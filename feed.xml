<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://qwertyyuiop1234.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://qwertyyuiop1234.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-17T17:37:19+00:00</updated><id>https://qwertyyuiop1234.github.io/feed.xml</id><title type="html">Eren Arxiv.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Deep Dive into MicroGPT by Karpathy</title><link href="https://qwertyyuiop1234.github.io/blog/2026/microgpt-karpathy/" rel="alternate" type="text/html" title="Deep Dive into MicroGPT by Karpathy"/><published>2026-02-17T17:00:00+00:00</published><updated>2026-02-17T17:00:00+00:00</updated><id>https://qwertyyuiop1234.github.io/blog/2026/microgpt-karpathy</id><content type="html" xml:base="https://qwertyyuiop1234.github.io/blog/2026/microgpt-karpathy/"><![CDATA[<p>In this post, I’m reviewing <a href="https://karpathy.github.io/2026/02/12/microgpt/" target="\_blank"><strong>MicroGPT by Andrej Karpathy</strong></a>, which is an essential resource for anyone interested in understanding how language models work from scratch.</p> <hr/> <h1 id="1-getting-datasets--tokenizer">1. Getting Datasets &amp; Tokenizer</h1> <h2 id="11-download-dataset">1.1 Download Dataset</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let there be a Dataset `docs`: list[str] of documents (e.g. a list of names)
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="sh">'</span><span class="s">input.txt</span><span class="sh">'</span><span class="p">):</span>
    <span class="kn">import</span> <span class="n">urllib.request</span>
    <span class="n">names_url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt</span><span class="sh">'</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlretrieve</span><span class="p">(</span><span class="n">names_url</span><span class="p">,</span> <span class="sh">'</span><span class="s">input.txt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">input.txt</span><span class="sh">'</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
<span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">num docs: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># total number of names is 32,033
</span></code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">urllib.request</code> is a Python standard library module for fetching data from URLs.</p> <p>The dataset consists of 32,033 human names, each on a separate line. After loading, the names are stripped of whitespace and shuffled randomly.</p> <p><br/></p> <h2 id="12-tokenizer">1.2 Tokenizer</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let there be a Tokenizer to translate strings to sequences of integers ("tokens") and back
</span><span class="n">uchars</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">docs</span><span class="p">)))</span>  <span class="c1"># unique characters in the dataset become token ids 0..n-1
</span><span class="n">BOS</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">uchars</span><span class="p">)</span>  <span class="c1"># token id for a special Beginning of Sequence (BOS) token
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">uchars</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># total number of unique tokens, +1 is for BOS
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">uchars</code> is the sorted set of all unique characters that appear across the names in <code class="language-plaintext highlighter-rouge">input.txt</code>. Each character is assigned a token ID from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">n-1</code>.</li> <li><code class="language-plaintext highlighter-rouge">vocab_size</code> is <code class="language-plaintext highlighter-rouge">len(uchars) + 1</code> because we need one additional ID for the <strong>BOS</strong> token. Every token — including special tokens — requires its own unique ID in the vocabulary.</li> </ul> <blockquote> <p><strong>What is BOS (Beginning of Sequence)?</strong></p> <p>BOS is a special token that marks the start of a sequence. During <strong>generation</strong>, the model receives the BOS token as its initial input to begin producing the first real character. Without BOS, the model would have no starting signal and could not initiate generation.</p> <p>In this character-level model, BOS is the <em>only</em> special token — and it <strong>doubles as the EOS (End of Sequence) token</strong>. Each name is wrapped like this:</p> <p><code class="language-plaintext highlighter-rouge">[BOS] e m m a [BOS]</code></p> <p>The same token appears at both the beginning and the end. During generation, when the model predicts BOS as the next token, that signals the name is complete and generation stops. This is an elegant design: a single special token handles both roles, keeping the vocabulary minimal.</p> </blockquote> <p>For a deeper understanding of tokenization strategies, see: <a href="https://nebius.com/blog/posts/how-tokenizers-work-in-ai-models" target="\_blank">How Tokenizers Work in AI Models (Nebius)</a></p> <hr/> <h1 id="2-computation-graph">2. Computation Graph</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let there be Autograd to recursively apply the chain rule through a computation graph
</span><span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
    <span class="n">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">grad</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">_children</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">_local_grads</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h2 id="21-why-__slots__">2.1 Why <code class="language-plaintext highlighter-rouge">__slots__</code>?</h2> <p>In a standard Python class, instance attributes are stored in a per-instance <code class="language-plaintext highlighter-rouge">__dict__</code> dictionary. This is flexible but comes with overhead: each dictionary maintains a hash table, keys, values, and internal bookkeeping.</p> <p><strong>Without <code class="language-plaintext highlighter-rouge">__slots__</code></strong> (default):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>instance
  └── __dict__  (a full hash-table dictionary)
        ├── key → value
        ├── key → value
        └── ...
</code></pre></div></div> <p><strong>With <code class="language-plaintext highlighter-rouge">__slots__</code></strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>instance
  └── fixed attribute slots  (C-struct-like, no dictionary)
        ├── data
        ├── grad
        ├── _children
        └── _local_grads
</code></pre></div></div> <p>By declaring <code class="language-plaintext highlighter-rouge">__slots__</code>, Python allocates a <strong>fixed-size struct</strong> for the listed attributes instead of a dictionary. This:</p> <ul> <li><strong>Saves memory</strong> — no dictionary overhead per instance (important when creating thousands of <code class="language-plaintext highlighter-rouge">Value</code> nodes in a computation graph).</li> <li><strong>Speeds up attribute access</strong> — direct offset-based lookup instead of hash-table lookup.</li> <li><strong>Prevents accidental attribute creation</strong> — assigning to a non-declared attribute raises <code class="language-plaintext highlighter-rouge">AttributeError</code>.</li> </ul> <p><br/></p> <h2 id="22-special-methods">2.2 Special Methods</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">children</span><span class="o">=</span><span class="p">(),</span> <span class="n">local_grads</span><span class="o">=</span><span class="p">()):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>                <span class="c1"># scalar value of this node (computed during forward pass)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>                   <span class="c1"># derivative of loss w.r.t. this node (computed during backward pass)
</span>    <span class="n">self</span><span class="p">.</span><span class="n">_children</span> <span class="o">=</span> <span class="n">children</span>       <span class="c1"># children of this node in the computation graph
</span>    <span class="n">self</span><span class="p">.</span><span class="n">_local_grads</span> <span class="o">=</span> <span class="n">local_grads</span> <span class="c1"># local derivatives of this node w.r.t. its children
</span>
<span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">__pow__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="o">**</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="p">(</span><span class="n">other</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="o">**</span><span class="p">(</span><span class="n">other</span><span class="o">-</span><span class="mi">1</span><span class="p">),))</span>
<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>  <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>  <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">),))</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="k">return</span> <span class="nc">Value</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="p">(</span><span class="nf">float</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">),))</span>
<span class="k">def</span> <span class="nf">__neg__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>      <span class="k">return</span> <span class="n">self</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">def</span> <span class="nf">__radd__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="n">self</span> <span class="o">+</span> <span class="n">other</span>
<span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="k">return</span> <span class="n">self</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">__rsub__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="n">other</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">__rmul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="n">self</span> <span class="o">*</span> <span class="n">other</span>
<span class="k">def</span> <span class="nf">__truediv__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="k">return</span> <span class="n">self</span> <span class="o">*</span> <span class="n">other</span><span class="o">**-</span><span class="mi">1</span>
<span class="k">def</span> <span class="nf">__rtruediv__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="n">other</span> <span class="o">*</span> <span class="n">self</span><span class="o">**-</span><span class="mi">1</span>
</code></pre></div></div> <p>Each operation creates a new <code class="language-plaintext highlighter-rouge">Value</code> node that records:</p> <ol> <li>The <strong>forward result</strong> (<code class="language-plaintext highlighter-rouge">data</code>)</li> <li>Its <strong>parent operands</strong> (<code class="language-plaintext highlighter-rouge">_children</code>)</li> <li>The <strong>local gradients</strong> (<code class="language-plaintext highlighter-rouge">_local_grads</code>) — partial derivatives of this operation’s output with respect to each child</li> </ol> <p>This builds a <strong>computation graph</strong> during the forward pass that will later be traversed in reverse for backpropagation.</p> <p><br/></p> <h3 id="221-normal-methods-vs-reflected-methods">2.2.1 Normal Methods vs. Reflected Methods</h3> <p>Consider <code class="language-plaintext highlighter-rouge">__add__</code> and <code class="language-plaintext highlighter-rouge">__radd__</code>. Why do we need the reflected (reverse) version?</p> <p>When Python evaluates <code class="language-plaintext highlighter-rouge">3 + Value(5)</code>:</p> <ol> <li>Python first tries <code class="language-plaintext highlighter-rouge">int.__add__(3, Value(5))</code> → <code class="language-plaintext highlighter-rouge">int</code> doesn’t know how to add a <code class="language-plaintext highlighter-rouge">Value</code>, so it returns <code class="language-plaintext highlighter-rouge">NotImplemented</code>.</li> <li>Python then falls back to <code class="language-plaintext highlighter-rouge">Value.__radd__(Value(5), 3)</code> → this works because <code class="language-plaintext highlighter-rouge">__radd__</code> calls <code class="language-plaintext highlighter-rouge">self + other</code>, which triggers <code class="language-plaintext highlighter-rouge">Value.__add__</code>.</li> </ol> <p>Without <code class="language-plaintext highlighter-rouge">__radd__</code>, expressions like <code class="language-plaintext highlighter-rouge">3 + Value(5)</code> or <code class="language-plaintext highlighter-rouge">2 * Value(3)</code> would raise a <code class="language-plaintext highlighter-rouge">TypeError</code>.</p> <p><br/></p> <h3 id="222-why-__sub__-uses---other-instead-of-direct-subtraction">2.2.2 Why <code class="language-plaintext highlighter-rouge">__sub__</code> Uses <code class="language-plaintext highlighter-rouge">+ (-other)</code> Instead of Direct Subtraction</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span> <span class="k">return</span> <span class="n">self</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">)</span>
</code></pre></div></div> <p>This might look roundabout, but it’s a deliberate design choice:</p> <ul> <li><strong>Reuses existing operations</strong>: Subtraction is decomposed into negation (<code class="language-plaintext highlighter-rouge">__neg__</code>) and addition (<code class="language-plaintext highlighter-rouge">__add__</code>), both of which are already implemented with correct gradient tracking.</li> <li><strong>Avoids redundant gradient logic</strong>: If subtraction were implemented as a separate operation, we would need to define and maintain additional local gradient formulas.</li> <li><strong>Keeps the computation graph simple</strong>: Every operation in the graph maps to one of a small set of primitives (<code class="language-plaintext highlighter-rouge">add</code>, <code class="language-plaintext highlighter-rouge">mul</code>, <code class="language-plaintext highlighter-rouge">pow</code>, <code class="language-plaintext highlighter-rouge">log</code>, <code class="language-plaintext highlighter-rouge">exp</code>, <code class="language-plaintext highlighter-rouge">relu</code>), making the backward pass straightforward.</li> </ul> <p><br/></p> <hr/> <p><em>This post will be updated as I continue studying the remaining sections of MicroGPT.</em></p>]]></content><author><name></name></author><category term="AI"/><category term="NLP"/><category term="microgpt"/><category term="karpathy"/><category term="autograd"/><category term="backpropagation"/><category term="nlp"/><summary type="html"><![CDATA[A detailed walkthrough of Karpathy's MicroGPT, covering dataset preparation, character-level tokenization, a minimal autograd engine (the Value class), Python special methods, and backpropagation via topological sort.]]></summary></entry><entry><title type="html">[CS231n] Assignment 1 - Q2. Implement a Softmax Classifier</title><link href="https://qwertyyuiop1234.github.io/blog/2026/cs231n-softmax/" rel="alternate" type="text/html" title="[CS231n] Assignment 1 - Q2. Implement a Softmax Classifier"/><published>2026-02-16T12:00:00+00:00</published><updated>2026-02-16T12:00:00+00:00</updated><id>https://qwertyyuiop1234.github.io/blog/2026/cs231n-softmax</id><content type="html" xml:base="https://qwertyyuiop1234.github.io/blog/2026/cs231n-softmax/"><![CDATA[<h1 id="1-preprocessing-datasets">1. Preprocessing Datasets</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># preprocessing: subtract the mean image
# first: compute the image mean based on the training data
</span><span class="n">mean_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">mean_image</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># print a few of the elements
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">mean_image</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">uint8</span><span class="sh">'</span><span class="p">))</span> <span class="c1"># visualize the mean image
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># second: subtract the mean image from train and test data
</span><span class="n">X_train</span> <span class="o">-=</span> <span class="n">mean_image</span>
<span class="n">X_val</span> <span class="o">-=</span> <span class="n">mean_image</span>
<span class="n">X_test</span> <span class="o">-=</span> <span class="n">mean_image</span>
<span class="n">X_dev</span> <span class="o">-=</span> <span class="n">mean_image</span>
</code></pre></div></div> <p>We have already flattened the <code class="language-plaintext highlighter-rouge">X_train</code> dataset in the previous code, so <code class="language-plaintext highlighter-rouge">X_train</code> $\in \mathbb{R}^{49000 \times 3072}$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/cs231n_softmax/Pasted%20image%2020260215223443-480.webp 480w,/assets/img/posts/cs231n_softmax/Pasted%20image%2020260215223443-800.webp 800w,/assets/img/posts/cs231n_softmax/Pasted%20image%2020260215223443-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/cs231n_softmax/Pasted%20image%2020260215223443.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p><em>Why subtract the mean image from the dataset?</em></p> <ul> <li><strong>Center the data</strong>: Makes the average of each feature zero. <ul> <li>Stabilizes training and helps gradient descent converge faster.</li> </ul> </li> <li><strong>Normalize brightness</strong>: Eliminates the common bias of “brightness” shared across images, allowing the model to focus on <strong>structural differences</strong>.</li> </ul> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># third: append the bias dimension of ones (i.e. bias trick) so that our classifier
# only has to worry about optimizing a single weight matrix W.
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X_val</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_val</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
<span class="n">X_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X_dev</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_dev</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</code></pre></div></div> <p>Here is an important trick for handling the bias term $b$. Recall the linear score function: \(f(x_i, W, b) = W x_i + b\) Computating $W$ and $b$ separately requires maintaining two distinct parameters (matrices and vectors), which complicates the implementation of gradients and updates.</p> <p>We can use the <strong>Bias Trick</strong> to simplify this:</p> \[\mathbf{x}' = \begin{bmatrix} \mathbf{x} \\ 1 \end{bmatrix} \in \mathbb{R}^{D+1}, \qquad \mathbf{W}' = \begin{bmatrix} \mathbf{W} &amp; \mathbf{b} \end{bmatrix} \in \mathbb{R}^{C \times (D+1)}\] <p><em>(Note: Dimensions may transpose depending on implementation. In our code, $X$ is $N \times D$, so we append 1 horizontally)</em></p> \[X' = \begin{bmatrix} X &amp; \mathbf{1} \end{bmatrix} \in \mathbb{R}^{N \times (D+1)}, \quad W' = \begin{bmatrix} W \\ b^\top \end{bmatrix} \in \mathbb{R}^{(D+1) \times C}\] \[f(X') = X'W' = XW + \mathbf{1}b^\top = XW + b \in \mathbb{R}^{N \times C}\] <p>By appending a column of ones to $X$, we absorb the bias $b$ into the weight matrix $W$. Now we only need to maintain a single matrix $W$.</p> <hr/> <h1 id="2-implementing-softmax_loss_naive">2. Implementing softmax_loss_naive</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#####################################################################
# TODO:
# Compute the gradient of the loss function and store it dW.
# Rather that first computing the loss and then computing the 
# derivative,      
# it may be simpler to compute the derivative at the same time that 
# the loss is being computed. As a result you may need to modify some 
# of the code above to compute the gradient.
#####################################################################
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Softmax loss function, naive implementation (with loops)
    Inputs have dimension D, there are C classes, and we operate on minibatches
    of N examples.
    
    Inputs:
    - W: A numpy array of shape (D, C) containing weights.
    - X: A numpy array of shape (N, D) containing a minibatch of data.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c means
         that X[i] has label c, where 0 &lt;= c &lt; C.
    - reg: (float) regularization strength
      
    Returns a tuple of:
    - loss as single float
    - gradient with respect to weights W; an array of same shape as W
    </span><span class="sh">"""</span>
    <span class="c1"># Initialize the loss and gradient to zero.
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    
    <span class="c1"># compute the loss and the gradient
</span>    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>  <span class="c1"># shape: (C,)
</span>        
        <span class="c1"># compute the probabilities in a numerically stable way
</span>        <span class="c1"># shift values for 'exp' to avoid numeric overflow
</span>        <span class="n">scores</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        
        <span class="c1"># Softmax Function
</span>        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">/=</span> <span class="n">p</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="c1"># normalize
</span>        <span class="n">logp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">-=</span> <span class="n">logp</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="c1"># negative log probability is the loss
</span>        
        <span class="c1"># My Code
</span>        <span class="n">p</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">dW</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">p</span><span class="p">)</span>
    
    <span class="c1"># normalized hinge loss plus regularization
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">num_train</span> <span class="o">+</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
    
    <span class="c1"># My Code
</span>    <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">/</span> <span class="n">num_train</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
    
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div> <p>Before implementing the gradient code, let’s derive the gradient of the <strong>Softmax Loss</strong>.</p> <p>The full loss function including regularization is:</p> \[L(W) = \frac{1}{N}\sum_{i=1}^{N} L_i(x_i, y_i, W) + \lambda R(W)\] <p>Where for a single example $x_i$:</p> <ul> <li>Scores: $f = X_i W$ (where $f_k = X_i W_k$)</li> <li>Softmax Probability: $p_k = \frac{e^{f_k}}{\sum_j e^{f_j}}$</li> <li>Loss: $L_i = -\log(p_{y_i})$</li> </ul> <hr/> <h2 id="21-analytical-gradient-derivation">2.1 Analytical Gradient Derivation</h2> <p>Our goal is to find $\frac{\partial L_i}{\partial W}$. We use the chain rule: \(\frac{\partial L_i}{\partial W} = \frac{\partial L_i}{\partial f} \frac{\partial f}{\partial W}\)</p> <h3 id="step-1-derivative-wrt-scores-f_k"><mark>Step 1: Derivative w.r.t Scores ($f_k$)</mark></h3> <p>Let $k$ be any class index. We want $\frac{\partial L_i}{\partial f_k}$.</p> <p>Since $L_i = -\log(p_{y_i})$, we differentiating $-\log(\frac{e^{f_{y_i}}}{\sum e^{f_j}})$.</p> <p><em><strong>Case 1</strong></em> : $k = y_i$ (Correct Class)</p> \[\begin{aligned} \frac{\partial L_i}{\partial f_{y_i}} &amp;= \frac{\partial}{\partial f_{y_i}} (-\log p_{y_i}) = -\frac{1}{p_{y_i}} \frac{\partial p_{y_i}}{\partial f_{y_i}} \\ &amp;= -\frac{1}{p_{y_i}} [p_{y_i}(1-p_{y_i})] = -(1-p_{y_i}) = p_{y_i} - 1 \end{aligned}\] <p><br/></p> <p><em><strong>Case 2</strong></em> : $k \neq y_i$ (Incorrect Class)</p> \[\begin{aligned} \frac{\partial L_i}{\partial f_k} &amp;= \frac{\partial}{\partial f_k} (-\log p_{y_i}) = -\frac{1}{p_{y_i}} \frac{\partial p_{y_i}}{\partial f_k} \\ &amp;= -\frac{1}{p_{y_i}} [-p_{y_i}p_k] = p_k \end{aligned}\] <p><br/></p> <p><em><strong>Combine Cases</strong></em> : We can combine these into a single expression using <strong>the indicator function</strong> $\mathbb{1}(k=y_i)$:</p> \[\frac{\partial L_i}{\partial f_k} = p_k - \mathbb{1}(k=y_i)\] <p>This is simply the <strong>probability vector minus the one-hot label vector</strong>. Let $\delta = p - y_{\text{onehot}}$. Then $\frac{\partial L_i}{\partial f} = \delta$.</p> <p><br/></p> <h3 id="step-2-derivative-wrt-weights-w"><mark>Step 2: Derivative w.r.t Weights ($W$)</mark></h3> <p>The score for class $k$ is $f_k = x_i \cdot W_k$ (dot product).</p> <p>So, $\frac{\partial f_k}{\partial W_{d,k}} = x_{i,d}$. (Note: $f_k$ only depends on the $k$-th column of $W$).</p> <p>Thus, for a specific weight element $W_{d,k}$:</p> \[\frac{\partial L_i}{\partial W_{d,k}} = \frac{\partial L_i}{\partial f_k} \frac{\partial f_k}{\partial W_{d,k}} = (p_k - \mathbb{1}(k=y_i)) x_{i,d}\] <p><br/></p> <h3 id="step-3-vectorized-gradient"><mark>Step 3: Vectorized Gradient</mark></h3> <p>Collecting this for all $d, k$, we get the outer product: \(\nabla_W L_i = x_i^T (p - y_{\text{onehot}})\)</p> <p><em>(Dimension check: $x_i^T$ is $(D,1)$, $(p-y)$ is $(1,C)$ $\rightarrow$ $(D,C)$ matrix)</em>.</p> <hr/> <h2 id="22-final-gradient-formula">2.2 Final Gradient Formula</h2> <p>Averaging over $N$ examples and adding regularization:</p> \[\nabla_W L(W) = \frac{1}{N} \sum_{i=1}^{N} x_i^T (p_i - y_i^{\text{onehot}}) + 2\lambda W\] <hr/> <h2 id="23-evaluation">2.3 Evaluation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate the naive implementation of the loss we provided for you:
</span><span class="kn">from</span> <span class="n">cs231n.classifiers.softmax</span> <span class="kn">import</span> <span class="n">softmax_loss_naive</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="c1"># generate a random Softmax classifier weight matrix of small numbers
</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3073</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="nf">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">loss: %f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># As a rough sanity check, our loss should be something close to -log(0.1).
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">loss: %f</span><span class="sh">'</span> <span class="o">%</span> <span class="n">loss</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">sanity check: %f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)))</span>
</code></pre></div></div> <p><br/></p> <h2 id="24-inline-question-1">2.4 Inline Question 1</h2> <blockquote> <p><mark>Why do we expect our loss to be close to -log(0.1)?</mark></p> <details> <summary><b>Answer</b></summary> Since $W$ is initialized with very small random numbers ($\approx 0$), the scores $f = XW$ will be approximately $0$. Consequently, the Softmax probabilities will be uniform across all classes: $$ p_k = \frac{e^{f_k}}{\sum_j e^{f_j}} \approx \frac{e^0}{\sum_{j=1}^{10} e^0} = \frac{1}{10} = 0.1 $$ The loss for each example is $Li = -\log(p_{y_i}) \approx -\log(0.1)$. </details> </blockquote>]]></content><author><name></name></author><category term="AI"/><category term="Computer Vision"/><category term="cs231n"/><category term="softmax"/><category term="cv"/><summary type="html"><![CDATA[A walkthrough of implementing a Softmax Classifier for CS231n Assignment 1, including preprocessing, loss function derivation, and gradient computation.]]></summary></entry></feed>